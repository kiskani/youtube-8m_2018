{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import gfile\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow import logging\n",
    "\n",
    "import os \n",
    "import time \n",
    "import utils \n",
    "import eval_util\n",
    "import readers \n",
    "import losses \n",
    "\n",
    "import video_level_models\n",
    "import frame_level_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = {} \n",
    "\n",
    "model_name = \"test_1\"\n",
    "\n",
    "flags[\"feature_names\"]          =  \"mean_rgb\" \n",
    "flags[\"feature_sizes\"]          =  \"1024\" \n",
    "flags[\"train_dir\"]              =  \"/Users/mohsenkiskani/yt8m/v2/models/video/\"+model_name   \n",
    "flags[\"train_data_pattern\"]     =  \"/Users/mohsenkiskani/yt8m/v2/video/train*.tfrecord\" \n",
    "flags[\"batch_size\"]             =  1024 \n",
    "flags[\"regularization_penalty\"] =  1.0 \n",
    "flags[\"base_learning_rate\"]     =  0.01 \n",
    "flags[\"learning_rate_decay\"]    =  0.95 \n",
    "flags[\"num_epochs\"]             =  5\n",
    "flags[\"num_readers\"]            =  8\n",
    "flags[\"clip_gradient_norm\"]     =  1.0\n",
    "flags[\"log_device_placement\"]   =  False\n",
    "flags[\"learning_rate_decay_examples\"] =  4000000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data_tensors(reader, data_pattern, batch_size=1000, num_epochs = None, num_readers=1):\n",
    "    files = gfile.Glob(data_pattern)\n",
    "    filename_queue = tf.train.string_input_producer(files, num_epochs=num_epochs, shuffle=True)\n",
    "    training_data = [reader.prepare_reader(filename_queue) for _ in range(num_readers)]\n",
    "    return tf.train.shuffle_batch_join(training_data,\n",
    "                                batch_size=batch_size, \n",
    "                                capacity=5 * batch_size, \n",
    "                                min_after_dequeue= batch_size,\n",
    "                                allow_smaller_final_batch=True,\n",
    "                                enqueue_many=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(reader, model, train_data_pattern, label_loss = losses.CrossEntropyLoss(), \n",
    "                batch_size=1000, base_learning_rate=0.01, learning_rate_decay_examples=1000000,\n",
    "                learning_rate_decay=0.95, optimizer_class=tf.train.AdamOptimizer, clip_gradient_norm=1.0,\n",
    "                regularization_penalty=1, num_readers=1, num_epochs=None):\n",
    "    \n",
    "    global_step   = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(base_learning_rate, global_step * batch_size,\n",
    "                                               learning_rate_decay_examples, learning_rate_decay, staircase=True)\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "    optimizer     = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    unused_video_id, model_input_raw, labels_batch, num_frames = (get_input_data_tensors(\n",
    "          reader,\n",
    "          train_data_pattern,\n",
    "          batch_size=batch_size ,\n",
    "          num_readers=num_readers,\n",
    "          num_epochs=num_epochs))\n",
    "    \n",
    "    tf.summary.histogram(\"model/input_raw\", model_input_raw)\n",
    "    feature_dim   = len(model_input_raw.get_shape()) - 1\n",
    "    model_input   = tf.nn.l2_normalize(model_input_raw, feature_dim)\n",
    "    \n",
    "    tower_label_losses = []\n",
    "    tower_reg_losses   = []\n",
    "    tower_gradients    = []\n",
    "    tower_predictions  = []\n",
    "\n",
    "    with (slim.arg_scope([slim.model_variable, slim.variable])):\n",
    "        result = model.create_model(\n",
    "                    model_input=model_input_raw,\n",
    "                    num_frames=num_frames,\n",
    "                    vocab_size=reader.num_classes,\n",
    "                    labels=labels_batch)\n",
    "\n",
    "        for variable in slim.get_model_variables():\n",
    "            tf.summary.histogram(variable.op.name, variable)\n",
    "\n",
    "        predictions   = result[\"predictions\"]\n",
    "        tower_predictions.append(predictions)\n",
    "        label_loss_fn = losses.CrossEntropyLoss()\n",
    "        label_loss    = label_loss_fn.calculate_loss(predictions, labels_batch)\n",
    "        reg_loss      = tf.constant(0.0)\n",
    "        reg_losses    = tf.losses.get_regularization_losses()\n",
    "\n",
    "        if reg_losses:\n",
    "            reg_loss += tf.add_n(reg_losses)\n",
    "\n",
    "        tower_reg_losses.append(reg_loss)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        if update_ops:\n",
    "                with tf.control_dependencies(update_ops):\n",
    "                    barrier = tf.no_op(name=\"gradient_barrier\")\n",
    "                    with tf.control_dependencies([barrier]):\n",
    "                        label_loss = tf.identity(label_loss)\n",
    "\n",
    "        tower_label_losses.append(label_loss)\n",
    "        final_loss = flags[\"regularization_penalty\"] * reg_loss + label_loss\n",
    "        gradients = optimizer.compute_gradients(final_loss, colocate_gradients_with_ops=False)\n",
    "        tower_gradients.append(gradients)\n",
    "\n",
    "    label_loss = tf.reduce_mean(tf.stack(tower_label_losses))\n",
    "    tf.summary.scalar(\"label_loss\", label_loss)\n",
    "    if flags[\"regularization_penalty\"] != 0:\n",
    "        reg_loss = tf.reduce_mean(tf.stack(tower_reg_losses))\n",
    "        tf.summary.scalar(\"reg_loss\", reg_loss)\n",
    "    merged_gradients = utils.combine_gradients(tower_gradients)\n",
    "\n",
    "    if flags[\"clip_gradient_norm\"] > 0:\n",
    "        merged_gradients = utils.clip_gradient_norms(merged_gradients, flags[\"clip_gradient_norm\"])\n",
    "\n",
    "    train_op = optimizer.apply_gradients(merged_gradients, global_step=global_step)\n",
    "\n",
    "    tf.add_to_collection(\"global_step\", global_step)\n",
    "    tf.add_to_collection(\"loss\", label_loss)\n",
    "    tf.add_to_collection(\"predictions\", tf.concat(tower_predictions, 0))\n",
    "    tf.add_to_collection(\"input_batch_raw\", model_input_raw)\n",
    "    tf.add_to_collection(\"input_batch\", model_input)\n",
    "    tf.add_to_collection(\"num_frames\", num_frames)\n",
    "    tf.add_to_collection(\"labels\", tf.cast(labels_batch, tf.float32))\n",
    "    tf.add_to_collection(\"train_op\", train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, max_steps_reached = False, max_steps = None ):\n",
    "    \n",
    "    feature_names, feature_sizes = utils.GetListOfFeatureNamesAndSizes(\n",
    "                                   flags[\"feature_names\"],flags[\"feature_sizes\"])\n",
    "    reader = readers.YT8MAggregatedFeatureReader(feature_names=feature_names, \n",
    "                                                 feature_sizes=feature_sizes)\n",
    "    \n",
    "    train_dir = flags[\"train_dir\"]\n",
    "    if not os.path.exists(train_dir):\n",
    "          os.makedirs(train_dir)\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "\n",
    "        saver =  build_graph(reader=reader,\n",
    "                             model=model,\n",
    "                             clip_gradient_norm=flags[\"clip_gradient_norm\"],\n",
    "                             train_data_pattern=flags[\"train_data_pattern\"],\n",
    "                             base_learning_rate=flags[\"base_learning_rate\"],\n",
    "                             learning_rate_decay=flags[\"learning_rate_decay\"],\n",
    "                             learning_rate_decay_examples=flags[\"learning_rate_decay_examples\"],\n",
    "                             regularization_penalty=flags[\"regularization_penalty\"],\n",
    "                             num_readers=flags[\"num_readers\"],\n",
    "                             batch_size=flags[\"batch_size\"],\n",
    "                             num_epochs=flags[\"num_epochs\"])\n",
    "        \n",
    "        global_step = tf.get_collection(\"global_step\")[0]\n",
    "        loss = tf.get_collection(\"loss\")[0]\n",
    "        predictions = tf.get_collection(\"predictions\")[0]\n",
    "        labels = tf.get_collection(\"labels\")[0]\n",
    "        train_op = tf.get_collection(\"train_op\")[0]\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        \n",
    "    sv = tf.train.Supervisor(graph, logdir= train_dir, init_op=init_op, global_step=global_step,\n",
    "                             save_model_secs=15 * 60, save_summaries_secs=120, saver=saver)\n",
    "\n",
    "    config = tf.ConfigProto(allow_soft_placement=True,log_device_placement=flags[\"log_device_placement\"])\n",
    "    \n",
    "    with sv.managed_session(\"\", config=config) as sess:\n",
    "        try:\n",
    "            while (not sv.should_stop()) and (not max_steps_reached):\n",
    "                batch_start_time = time.time()\n",
    "                _, global_step_val, loss_val, predictions_val, labels_val = sess.run(\n",
    "                    [train_op, global_step, loss, predictions, labels])\n",
    "                seconds_per_batch = time.time() - batch_start_time\n",
    "                examples_per_second = labels_val.shape[0] / seconds_per_batch\n",
    "\n",
    "                if max_steps and max_steps <= global_step_val:\n",
    "                    max_steps_reached = True\n",
    "\n",
    "                if global_step_val % 10 == 0 and train_dir:\n",
    "                    eval_start_time = time.time()\n",
    "                    hit_at_one = eval_util.calculate_hit_at_one(predictions_val, labels_val)\n",
    "                    perr = eval_util.calculate_precision_at_equal_recall_rate(predictions_val, labels_val)\n",
    "                    gap = eval_util.calculate_gap(predictions_val, labels_val)\n",
    "                    eval_end_time = time.time()\n",
    "                    eval_time = eval_end_time - eval_start_time\n",
    "\n",
    "                    logging.info(\"training step \" + str(global_step_val) + \" | Loss: \" + (\"%.2f\" % loss_val) +\n",
    "                    \" Examples/sec: \" + (\"%.2f\" % examples_per_second) + \" | Hit@1: \" +\n",
    "                    (\"%.2f\" % hit_at_one) + \" PERR: \" + (\"%.2f\" % perr) +\n",
    "                    \" GAP: \" + (\"%.2f\" % gap))\n",
    "\n",
    "                    sv.summary_writer.add_summary(utils.MakeSummary(\"model/Training_Hit@1\", hit_at_one),\n",
    "                                                  global_step_val)\n",
    "                    sv.summary_writer.add_summary(utils.MakeSummary(\"model/Training_Perr\", perr), \n",
    "                                                  global_step_val)\n",
    "                    sv.summary_writer.add_summary(utils.MakeSummary(\"model/Training_GAP\", gap), \n",
    "                                                  global_step_val)\n",
    "                    sv.summary_writer.add_summary(utils.MakeSummary(\"global_step/Examples/Second\", \n",
    "                                                                    examples_per_second), global_step_val)\n",
    "                    sv.summary_writer.flush()\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            logging.info(\"Done training -- epoch limit reached.\")\n",
    "\n",
    "    logging.info(\"Exited training loop.\")\n",
    "    sv.Stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticModel():\n",
    "    def create_model(self, model_input, vocab_size, l2_penalty=1e-8, **unused_params):\n",
    "        output = slim.fully_connected(model_input, vocab_size, activation_fn=tf.nn.sigmoid,\n",
    "                                      weights_regularizer=slim.l2_regularizer(l2_penalty))\n",
    "        return {\"predictions\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoeModel():\n",
    "    def create_model(self, model_input, vocab_size, num_mixtures=5, l2_penalty=1e-8, **unused_params):\n",
    "        \n",
    "        gate_activations = slim.fully_connected(model_input,\n",
    "                                                vocab_size * (num_mixtures + 1),\n",
    "                                                activation_fn=None,\n",
    "                                                biases_initializer=None,\n",
    "                                                weights_regularizer=slim.l2_regularizer(l2_penalty),\n",
    "                                                scope=\"gates\")\n",
    "        \n",
    "        expert_activations = slim.fully_connected(model_input,\n",
    "                                                  vocab_size * num_mixtures,\n",
    "                                                  activation_fn=None,\n",
    "                                                  weights_regularizer=slim.l2_regularizer(l2_penalty),\n",
    "                                                  scope=\"experts\")\n",
    "        \n",
    "        gating_distribution = tf.nn.softmax(tf.reshape(gate_activations,\n",
    "                                            [-1, num_mixtures + 1]))  # (Batch * #Labels) x (num_mixtures + 1)\n",
    "        expert_distribution = tf.nn.sigmoid(tf.reshape(expert_activations,\n",
    "                                            [-1, num_mixtures]))  # (Batch * #Labels) x num_mixtures\n",
    "        \n",
    "        final_probabilities_by_class_and_batch = tf.reduce_sum(\n",
    "            gating_distribution[:, :num_mixtures] * expert_distribution, 1)\n",
    "        \n",
    "        final_probabilities = tf.reshape(final_probabilities_by_class_and_batch, [-1, vocab_size])\n",
    "        \n",
    "        return {\"predictions\": final_probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasMoeModel():\n",
    "    def create_model(self, model_input, vocab_size, num_mixtures=5, l2_penalty=1e-8, **unused_params):\n",
    "        \n",
    "        gate_activations = Dense(vocab_size * (num_mixtures + 1), activation='softmax', \n",
    "                                 kernel_regularizer=regularizers.l2(l2_penalty))(model_input)\n",
    "        \n",
    "        expert_activations = Dense(vocab_size * num_mixtures, activation='sigmoid',\n",
    "                                   kernel_regularizer=regularizers.l2(l2_penalty))(model_input)\n",
    "        \n",
    "        #gating_distribution = keras.layers.Flatten(gate_activations)\n",
    "        #expert_distribution = keras.layers.Flatten(expert_activations)\n",
    "        \n",
    "        final_probabilities_by_class_and_batch = keras.layers.add(keras.layers.Multiply([\n",
    "            gate_activations[:, :num_mixtures], expert_activations]))\n",
    "        \n",
    "        final_probabilities = final_probabilities_by_class_and_batch\n",
    "        \n",
    "        #gating_distribution = tf.nn.softmax(tf.reshape(gate_activations,\n",
    "        #                                    [-1, num_mixtures + 1]))  # (Batch * #Labels) x (num_mixtures + 1)\n",
    "        #expert_distribution = tf.nn.sigmoid(tf.reshape(expert_activations,\n",
    "        #                                    [-1, num_mixtures]))  # (Batch * #Labels) x num_mixtures\n",
    "        \n",
    "        #final_probabilities_by_class_and_batch = tf.reduce_sum(\n",
    "        #    gating_distribution[:, :num_mixtures] * expert_distribution, 1)\n",
    "        \n",
    "        #final_probabilities = tf.reshape(final_probabilities_by_class_and_batch, [-1, vocab_size])\n",
    "        \n",
    "        return {\"predictions\": final_probabilities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import plot_model\n",
    "#from keras.models import Model\n",
    "#from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "\n",
    "from keras import regularizers\n",
    "import keras \n",
    "\n",
    "class KerasLogisticModel():\n",
    "    def create_model(self, model_input, vocab_size, l2_penalty=1e-6, **unused_params):\n",
    "        #output = Dense(vocab_size, activation='sigmoid', \n",
    "        #               kernel_regularizer=regularizers.l2(l2_penalty))(model_input)\n",
    "        \n",
    "        output_1 = Conv2D(filters = [32, 32], kernel_size= 4, strides=1, padding='same', \n",
    "                          kernel_regularizer=regularizers.l2(l2_penalty))(model_input)\n",
    "        output_2 = Dense(vocab_size, activation='sigmoid', \n",
    "                         kernel_regularizer=regularizers.l2(l2_penalty))(output_1)\n",
    "        return {\"predictions\": output_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-51616d2fc193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasLogisticModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model = KerasMoeModel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-01ed25d4e075>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, max_steps_reached, max_steps)\u001b[0m\n\u001b[1;32m     22\u001b[0m                              \u001b[0mnum_readers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_readers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                              num_epochs=flags[\"num_epochs\"])\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"global_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-72022af16357>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(reader, model, train_data_pattern, label_loss, batch_size, base_learning_rate, learning_rate_decay_examples, learning_rate_decay, optimizer_class, clip_gradient_norm, regularization_penalty, num_readers, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     labels=labels_batch)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvariable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-475b6e03bb52>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(self, model_input, vocab_size, l2_penalty, **unused_params)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         output_1 = Conv2D(filters = [32, 32], kernel_size= 4, strides=1, padding='same', \n\u001b[0;32m---> 16\u001b[0;31m                           kernel_regularizer=regularizers.l2(l2_penalty))(model_input)\n\u001b[0m\u001b[1;32m     17\u001b[0m         output_2 = Dense(vocab_size, activation='sigmoid', \n\u001b[1;32m     18\u001b[0m                          kernel_regularizer=regularizers.l2(l2_penalty))(output_1)\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    472\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    475\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_1: expected ndim=4, found ndim=2"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(tf.logging.INFO)\n",
    "#model = LogisticModel()\n",
    "#model = MoeModel()\n",
    "model = KerasLogisticModel()\n",
    "#model = KerasMoeModel()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:Recording summary at step 0.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-53a95fc5700b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#model = MoeModel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model = KerasLogisticModel()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-01ed25d4e075>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, max_steps_reached, max_steps)\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0mhit_at_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_hit_at_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mperr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_precision_at_equal_recall_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                     \u001b[0mgap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_gap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                     \u001b[0meval_end_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0meval_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_end_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0meval_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yt8m/code/youtube-8m/eval_util.py\u001b[0m in \u001b[0;36mcalculate_gap\u001b[0;34m(predictions, actuals, top_k)\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0msparse_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_positives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_k_by_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactuals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0mgap_calculator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_positives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgap_calculator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeek_ap_at_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yt8m/code/youtube-8m/average_precision_calculator.py\u001b[0m in \u001b[0;36mpeek_ap_at_n\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m                       \u001b[0mpredlists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                       \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                       total_num_positives=self._total_positives)\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/yt8m/code/youtube-8m/average_precision_calculator.py\u001b[0m in \u001b[0;36map_at_n\u001b[0;34m(predictions, actuals, n, total_num_positives)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         reverse=True)\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_num_positives\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.set_verbosity(tf.logging.INFO)\n",
    "model = LogisticModel()\n",
    "#model = MoeModel()\n",
    "#model = KerasLogisticModel()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
